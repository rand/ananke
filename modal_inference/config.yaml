# Ananke Modal Inference Configuration

# Service configuration
service:
  name: "ananke-inference"
  version: "0.1.0"
  description: "GPU-accelerated constrained code generation service"

# Model configuration
model:
  # Default model to use
  default: "llama-3.1-8b"

  # Available models (key -> model_name mapping)
  # Select based on VRAM requirements and use case
  available:
    llama-3.1-8b:
      name: "meta-llama/Meta-Llama-3.1-8B-Instruct"
      vram_gb: 16
      context_length: 128000
      recommended_use: "Development, testing, general code generation"

    llama-3.1-70b:
      name: "meta-llama/Meta-Llama-3.1-70B-Instruct"
      vram_gb: 80
      context_length: 128000
      recommended_use: "Production, complex constraints, high quality"

    mistral-7b:
      name: "mistralai/Mistral-7B-Instruct-v0.3"
      vram_gb: 14
      context_length: 32768
      recommended_use: "Fast inference, smaller context"

    deepseek-coder-6.7b:
      name: "deepseek-ai/deepseek-coder-6.7b-instruct"
      vram_gb: 13
      context_length: 16384
      recommended_use: "Code-specific tasks, efficient"

# GPU configuration
gpu:
  # GPU type: a100-40gb, a100-80gb
  type: "a100-40gb"
  count: 1

  # Memory utilization (0.0-1.0)
  memory_utilization: 0.90

  # For multi-GPU setups
  tensor_parallel_size: 1

# Container configuration
container:
  # Idle timeout in seconds (scale to zero)
  idle_timeout: 60

  # Maximum concurrent requests per container
  max_concurrent_inputs: 10

  # Request timeout in seconds
  timeout: 600

  # Python version
  python_version: "3.11"

# Generation defaults
generation:
  # Default sampling parameters
  temperature: 0.7
  top_p: 0.95
  top_k: 50
  max_tokens: 2048

  # Stop sequences
  stop_sequences:
    - "<|end|>"
    - "<|eot_id|>"
    - "</s>"

# Constraint configuration
constraints:
  # Constraint compilation timeout (ms)
  compile_timeout_ms: 5000

  # Supported constraint types
  supported_types:
    - "json"        # JSON schema constraints
    - "grammar"     # EBNF/custom grammars
    - "regex"       # Regular expression patterns
    - "composite"   # Combined constraints

  # Constraint validation
  validate_output: true
  strict_mode: true

# Logging configuration
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

  # Log specific events
  log_constraints: true
  log_generation_stats: true
  log_performance_metrics: true

# Performance tuning
performance:
  # Enable eager execution (required for llguidance)
  enforce_eager: true

  # Trust remote code (for some models)
  trust_remote_code: true

  # Auto-detect optimal dtype
  dtype: "auto"

  # KV cache optimization
  # max_model_len: null  # Auto-detect from model

# Security
security:
  # Require authentication token
  require_auth: false

  # Rate limiting
  rate_limit:
    enabled: false
    requests_per_minute: 60

# Monitoring
monitoring:
  # Track metrics
  enable_metrics: true

  # Metrics to track
  metrics:
    - "generation_time_ms"
    - "tokens_per_second"
    - "constraint_violations"
    - "request_count"
    - "error_rate"

# Development settings
development:
  # Use smaller model for local testing
  local_model: "llama-3.1-8b"

  # Reduced timeout for faster iteration
  local_timeout: 300

  # Enable debug logging
  debug: false

# Production settings
production:
  # Use larger model for quality
  model: "llama-3.1-70b"

  # Stricter timeouts
  timeout: 600

  # Enable all monitoring
  monitoring_enabled: true

  # Auto-scaling configuration
  autoscaling:
    min_containers: 0
    max_containers: 10
    target_concurrent_requests: 8
