# Ananke Modal Inference Service Dependencies
# Updated for vLLM 0.11.0 with llguidance support
# Based on working maze example configuration

# ============================================================================
# Modal Deployment
# ============================================================================
modal>=0.70.0

# ============================================================================
# Core vLLM Stack (installed in Modal container image)
# ============================================================================
# These are specified in the Modal image configuration, not installed locally
# Documented here for reference and version tracking

# vLLM and GPU dependencies
# vllm==0.11.0  # Stable version with llguidance support
# torch==2.9.0
# transformers==4.55.2  # Required by vLLM 0.11.0
# tokenizers>=0.21.1
# flashinfer-python==0.5.2

# Structured output backends
# llguidance>=0.7.11,<0.8.0  # CRITICAL: vLLM 0.11.0 requires <0.8.0
# xgrammar==0.1.25
# lm-format-enforcer==0.11.3
# outlines_core==0.2.11

# FastAPI and serving
# fastapi[standard]>=0.115.0
# pydantic>=2.12.0
# openai>=1.99.1
# aiohttp

# Performance and scaling
# ray[cgraph]>=2.48.0  # Required for pipeline parallelism in V1
# numba==0.61.2  # Required for N-gram speculative decoding

# Utilities
# protobuf
# tiktoken>=0.6.0
# sentencepiece
# pillow

# ============================================================================
# Client Dependencies (local)
# ============================================================================
requests>=2.31.0
urllib3>=2.5.0

# ============================================================================
# Development Tools
# ============================================================================
pytest>=8.0.0
black>=24.0.0
mypy>=1.8.0
